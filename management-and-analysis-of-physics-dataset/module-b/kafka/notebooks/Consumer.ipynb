{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2679d27",
   "metadata": {},
   "source": [
    "# Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "# -> 'local'\n",
    "# -> 'docker_cluster'\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "if CLUSTER_TYPE == 'local':\n",
    "\n",
    "    KAFKA_HOME = '<PATH_TO_YOUR_kafka_2.13-3.2.0_FOLDER>'\n",
    "    KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']\n",
    "    \n",
    "    # Start Zookeeper    \n",
    "    os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "    # Start one Kafka Broker\n",
    "    os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "\n",
    "    KAFKA_BOOTSTRAP_SERVERS = ['kafka-broker:9092',\"\"\" possibly other brokers in your kafka cluster \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aebe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install kafka-python confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bf5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1fd007",
   "metadata": {},
   "source": [
    "Kafka consumers can be instantiated via the KafkaConsumer class\n",
    "\n",
    "```python\n",
    "#--- A TYPICAL CONSUMER\n",
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers=['62.30.10.23:9092'],  #<<<--- list of brokers\n",
    "    security_protocol=\"SSL\",                 #<<<--- security protocol (if any) \n",
    "    ssl_cafile=\"./ca.pem\",                   #<<<--- certificate details (if any)\n",
    "    ssl_certfile=\"./service.cert\",           #           ...\n",
    "    ssl_keyfile=\"./service.key\",             #           ...\n",
    "    value_deserializer=msgpack.unpackb,      #<<<--- message value deserialization function (e.g. unpack the message from a specific format)\n",
    "    auto_offset_reset='earliest',            #<<<--- automatically bring the reading offset to the earliest message\n",
    "    group_id=\"group_A\",                      #<<<--- identify this consumer as part of group_A\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "Once more we'll use a simple implementation of the consumer, with no specific configurations used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a consumer pointing to a kafka cluster\n",
    "consumer = KafkaConsumer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "                         consumer_timeout_ms=10000 # if no message is available for consumption \n",
    "                                                   # after 10s stop the consumer\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d27c9",
   "metadata": {},
   "source": [
    "Inspect the brokers for the available topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all available topics on the kafka brokers\n",
    "consumer.topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9fe31",
   "metadata": {},
   "source": [
    "In the PUB/SUB model, we first need to subscribe to the topic of choice.\n",
    "\n",
    "Subscribing doesn't mean any message is actually received/consumed... \n",
    "\n",
    "It only means that from now on the consumer will be able to poll from the partitions of the chosen topics hosted on the brokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subscribe to topic\n",
    "consumer.subscribe('my_awesome_topic')\n",
    "\n",
    "# and check the active subscriptions\n",
    "consumer.subscription()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9745516",
   "metadata": {},
   "source": [
    "The `consumer` class will also offer a possibility to inspect the topics (for instance in terms of the number of partitions), but **not** to modify them. \n",
    "\n",
    "We can inspect how many partitions the specific topic is made of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the list of partition IDs \n",
    "# e.g. a topic with tree partitions will have partition IDs {0, 1, 2}\n",
    "consumer.partitions_for_topic('my_awesome_topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac50e4a",
   "metadata": {},
   "source": [
    "We can instruct the consumer to `poll` (i.e. ask for new messages from the topic) with a given cadence/logic.\n",
    "\n",
    "For instance we can set the consumer to read out only 10 messages at a time, with a timeout between to subsequent readouts of a given $\\Delta t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211359b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the polling strategy for the consumer\n",
    "consumer.poll(timeout_ms=0,         #<<--- do not enable dead-times before one poll to the next\n",
    "              max_records=None,     #<<--- do not limit the number of records to consume at once \n",
    "              update_offsets=True   #<<--- update the reading offsets on this topic\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b87dec",
   "metadata": {},
   "source": [
    "Now the consumer is ready to poll messages (untile it is stopped or it reaches a timeout).\n",
    "\n",
    "Let's look for all messages in the consumer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d5b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this consumer will keep polling and reading for messages until stopped (or it reaches the consumer_timeout_ms)\n",
    "for message in consumer:\n",
    "    print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a7a25",
   "metadata": {},
   "source": [
    "The reading offset can also be brought back to the beginning of the topic, to re-read the entire topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.seek_to_beginning()\n",
    "\n",
    "for message in consumer:\n",
    "    print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc220cc1",
   "metadata": {},
   "source": [
    "The message content (`ConsumerRecord`) is quite messy, but can be easily inspected parsing only the relevant infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "consumer.seek_to_beginning()\n",
    "\n",
    "# break down the message into its main components\n",
    "for message in consumer:\n",
    "    print (\"%d:%d [%s] k=%s v=%s\" % (message.partition,\n",
    "                          message.offset,\n",
    "                          datetime.fromtimestamp(message.timestamp/1000).time(),\n",
    "                          message.key,\n",
    "                          message.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9186d",
   "metadata": {},
   "source": [
    "Let's change the topic to which the consumer is subscribed to a partitioned one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.subscribe('a_partitioned_topic')\n",
    "consumer.subscription()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38f95e",
   "metadata": {},
   "source": [
    "By inspecting the number of partitions for this topic we do see now 2 partitions: partition #0 and partition #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.partitions_for_topic('a_partitioned_topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c87793",
   "metadata": {},
   "source": [
    "Reading out from a partitioned topic it's easy to see that the messages are sent to the two partitions in a seemengly arbitrary way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "consumer.seek_to_beginning()\n",
    "\n",
    "for message in consumer:\n",
    "    print (\"%d:%d:\\t v=%s\" % (message.partition,\n",
    "                          message.offset,\n",
    "                          json.loads(message.value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd766d",
   "metadata": {},
   "source": [
    "## Creating a consumer accessing only one partition\n",
    "\n",
    "Publishing records to a partitioned topic is typically transparent for the user: the producer publishes to the topic, and the kafka cluster will redirect the message to the partition leader, later replicating that to the followers.\n",
    "\n",
    "The same goes for a generic consumer. As we have just seen data is consumed from all partitions within the topic.\n",
    "\n",
    "In some cases, it can however be more suitable to instantiate multiple consumers, each reading from a specific partition of a topic.\n",
    "\n",
    "Let's assign a consumer specific to access the data of partition #0 of the previous partitioned topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import TopicPartition\n",
    "\n",
    "# create a standard consumer\n",
    "consumer_part_0 = KafkaConsumer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "                                client_id='consumer_n_0',\n",
    "                                consumer_timeout_ms=10000)\n",
    "\n",
    "# assign it to a specific topic - partition combination\n",
    "consumer_part_0.assign([TopicPartition('a_partitioned_topic', 0)]) # <<--- name of the topic, partition id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefaba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_part_0.seek_to_beginning()\n",
    "\n",
    "for message in consumer_part_0:\n",
    "    print (\"%d:%d:\\t v=%s\" % (message.partition,\n",
    "                          message.offset,\n",
    "                          json.loads(message.value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e776c",
   "metadata": {},
   "source": [
    "## Creating a consumer group\n",
    "\n",
    "Multiple consumers can read from the same topic.\n",
    "\n",
    "In kafka, every consumer is part of a consumer group (even a single consumer is part of its own consumer group, really). \n",
    "\n",
    "A consumer group is a number (1 or more) of cooperating consumers gathering data from the same topic, balancing the load across them and redistributing the consume calls dynamically.\n",
    "\n",
    "If a consumer inside a consumer-group fails, the others from the same group will keep reading the whole data from the topic to which they are subscribed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca71af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one consumer_one to read from 1 partition\n",
    "# assign this consumer to a group\n",
    "consumer_one = KafkaConsumer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "                             client_id='consumer_one',\n",
    "                             group_id='my_group',\n",
    "                             auto_offset_reset='earliest',\n",
    "                             consumer_timeout_ms=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import ConsumerRebalanceListener\n",
    "# subscribe this specific consumer to the partitioned topic\n",
    "consumer_one.subscribe('a_partitioned_topic',\n",
    "                       listener=ConsumerRebalanceListener())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd55dbc",
   "metadata": {},
   "source": [
    "#### BREAK\n",
    "Use the ConsumerGroup notebook to:\n",
    "1. create a second consumer `consumer_two`\n",
    "2. assign it to the same consumer group `my_group`\n",
    "3. subscribe to the same topic `a_partitioned_topic`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f527b",
   "metadata": {},
   "source": [
    "Each consumer within a group is going to be an independent process (should be run in parallel from the others) and will provide access to a fraction of the incoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7960448",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_one.assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multiple consumers in parallel --> typically you would run each on a different thread / process / executor\n",
    "for message in consumer_one:\n",
    "    print (\"%d:%d: k=%s v=%s\" % (message.partition,\n",
    "                          message.offset,\n",
    "                          message.key,\n",
    "                          json.loads(message.value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c771db",
   "metadata": {},
   "source": [
    "## Reading from the Kafka+Spark results topic\n",
    "\n",
    "Let's subscribe to the `results` topic and monitor the frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c33193",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.subscribe('results')\n",
    "\n",
    "for message in consumer:\n",
    "    print (\"%d:%d: k=%s v=%s\" % (message.partition,\n",
    "                          message.offset,\n",
    "                          message.key,\n",
    "                          message.value))\n",
    "    print ('      --> sending alert message to user {}\\n'.format(message.key.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a448c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
