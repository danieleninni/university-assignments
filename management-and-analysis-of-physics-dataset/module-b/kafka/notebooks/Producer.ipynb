{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37dfaa9",
   "metadata": {},
   "source": [
    "# Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "# -> 'local'\n",
    "# -> 'docker_cluster'\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c38f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "if CLUSTER_TYPE == 'local':\n",
    "\n",
    "    KAFKA_HOME = '\"\"\"PATH_TO_YOUR_kafka_2.13-3.2.0_FOLDER\"\"\"'\n",
    "    KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092',]\n",
    "    \n",
    "    # Start Zookeeper    \n",
    "    os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "    # Start one Kafka Broker\n",
    "    os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "\n",
    "    KAFKA_BOOTSTRAP_SERVERS = ['kafka-broker:9092',]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d54a37",
   "metadata": {},
   "source": [
    "## Interacting with Kafka Producer from shell\n",
    "\n",
    "The Producer is the Kafka abstraction for publishing data _into_ a Kafka topic.\n",
    "\n",
    "It is completely detached from any possible Consumer process, therefore there is no need to connect directly a Producer(Sender) and a Consumer(Receiver) in Kafka, due to its PUB/SUB messaging model.\n",
    "\n",
    "Hoever, in order for a Producer to publish a message, we need to specify at least 2 things:\n",
    "1. The topic to which the messages will be published\n",
    "2. The location of the cluster over the network\n",
    "\n",
    "Clearly, the goal is to work with Kafka programmatically (from python/C/...) to interface it with other possible applications.\n",
    "However, for having a first glance, we will interact with the Kafka cluster in _interactive mode_ from the shell.\n",
    "\n",
    "Apache Kafka provides a set of bash scripts to interact and operate with the cluster for basic operations and testing such as:\n",
    "- topics creation, configuration and inspection\n",
    "- shell-based message producer \n",
    "- shell-based message consumer\n",
    "- shell-based performance testing\n",
    "- ...\n",
    "\n",
    "Let's first conect to the kafka cluster from shell:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161de49",
   "metadata": {},
   "source": [
    "### For _docker_cluster_ users\n",
    "```console=\n",
    "$ docker ps\n",
    "\n",
    "!!! replace your_kafka-broker_procid with your own kafka-broker process id\n",
    "!!!                          ||| \n",
    "!!!                          vvv  \n",
    "$ docker exec -it <your_kafka-broker_procid> bash\n",
    "\n",
    "# cd /usr/bin/kafka_2.13-3.2.0/bin\n",
    "# ls\n",
    "# ./kafka-topics.sh --list --bootstrap-server kafka-broker:9092\n",
    "# ./kafka-topics.sh --create --topic my_awesome_topic --bootstrap-server kafka-broker:9092                \n",
    "# ./kafka-topics.sh --list --bootstrap-server kafka-broker:9092\n",
    "# ./kafka-topics.sh --describe --topic my_awesome_topic --bootstrap-server kafka-broker:9092\n",
    "# ./kafka-console-producer.sh --topic my_awesome_topic --bootstrap-server kafka-broker:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfba1a",
   "metadata": {},
   "source": [
    "### For _Local_ users\n",
    "```console=\n",
    "\n",
    "!!! replace KAFKA_HOME with your own path to kafka_2.13-3.2.0 folder\n",
    "!!!     ||| \n",
    "!!!     vvv  \n",
    "$ cd KAFKA_HOME/bin \n",
    "\n",
    "$ ls \n",
    "$ ./kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "$ ./kafka-topics.sh --create --topic my_awesome_topic --bootstrap-server localhost:9092                \n",
    "$ ./kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "$ ./kafka-topics.sh --describe --topic my_awesome_topic --bootstrap-server localhost:9092\n",
    "$ ./kafka-console-producer.sh --topic my_awesome_topic --bootstrap-server localhost:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443750b0",
   "metadata": {},
   "source": [
    "At this point you should be able to send messages to the topic you just created via the kafka-console-producer.\n",
    "\n",
    "So far, no consumer is available to process or even display those messages... \n",
    "Yet the messages are succesfully sent to the topic, increasing the log(s) in the (possibly more than one) partition(s).\n",
    "\n",
    "Let's create a console consumer and subscribe to the topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7ec7c",
   "metadata": {},
   "source": [
    "### For _docker_cluster_ users\n",
    "```console\n",
    "# ./kafka-console-consumer.sh --topic my_awesome_topic --bootstrap-server kafka-broker:9092 [--from-beginning]\n",
    "```\n",
    "\n",
    "### For _Local_ users\n",
    "```console\n",
    "$ ./kafka-console-consumer.sh --topic my_awesome_topic --bootstrap-server localhost:9092 [--from-beginning]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f1dab",
   "metadata": {},
   "source": [
    "## Programmatically produce messages to Kafka with Python\n",
    "\n",
    "Various python modules are available to interact with kafka programmatically, including:\n",
    "- kafka-python\n",
    "- confluent-kafka\n",
    "- pyKafka\n",
    "\n",
    "The differences between these modules are relatively minor. As always, take your time to glance at the documentation of all alternative before starting a project.\n",
    "\n",
    "We'll now use `kafka-python` to handle topics and producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kafka-python confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a7968",
   "metadata": {},
   "source": [
    "Kafka producers can be instantiated via the KafkaProducer class\n",
    "\n",
    "```python\n",
    "#--- A TYPICAL PRODUCER\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['62.30.10.23:9092'],  #<<<--- list of brokers\n",
    "    security_protocol=\"SSL\",                 #<<<--- security protocol (if any) \n",
    "    ssl_cafile=\"./ca.pem\",                   #<<<--- certificate details (if any)\n",
    "    ssl_certfile=\"./service.cert\",           #           ...\n",
    "    ssl_keyfile=\"./service.key\",             #           ...\n",
    "    value_serializer=msgpack.dumps           #<<<--- message value serialization function (e.g. interpred the message as a specific format)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "We'll play with the vanilla version of the producer.\n",
    "No certificates or specific serialization is used in this example.\n",
    "\n",
    "A simple producer instantiated by pointing it to the kafka brokers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "# create a simple producer pointing to the list of Kafka brokers we have created\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276dbb1",
   "metadata": {},
   "source": [
    "Let's try to publish a message to the topic we previously created without specifying any given key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send('my_awesome_topic', b'message 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d92ec3",
   "metadata": {},
   "source": [
    "The output message `<kafka.producer.future...>` is telling us explicitely that the record has been created and will be sent, at some point...\n",
    "\n",
    "However it has not been sent just yet when we pressed return on the previous cell.\n",
    "\n",
    "`KafkaProducer.send()` is in fact an _asynchronous_ publish method.\n",
    "\n",
    "This means that the producer will enqueue the message on an internal queue which is later (after a tunable max buffering time / given number of messages) sent to the broker if a leader is available, else wait some more time for it to respond.\n",
    "\n",
    "This behaviour is perfectly OK. \n",
    "Actually, not sending every single messge one by one, but packing more messages in small batches improves the data transfer dramatically.\n",
    "\n",
    "If we send 1 single message we won't even notice the difference, but imagine sending 7 trillions messages per day... It translates to roughly 81 millions messages per second. Optimizing the data transfer and minimizing overheads in the communication between the producers and the topics becomes relevant\n",
    "\n",
    "Just be aware that the messages won't necessarily be sent right away.\n",
    "If a large message rate is sent and `exit()` is issued right after a `producer.send()` command, it might occur that no message is actually sent, because the max of the buffering time/n.msg has not been reached before the `exit` of the program.\n",
    "\n",
    "Have a look at the API for all the tunable parameters: https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html\n",
    "\n",
    "\n",
    "To send a message \"synchronously\" it can be issued a `flush()` of the producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55004ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send('my_awesome_topic', b'a new message')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795d18f",
   "metadata": {},
   "source": [
    "It's important to realize that producers and consumers are completely decoupled. \n",
    "Even if a producer dies the consumer won't be affected by it, as it will still be able to access the topic on the brokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "producer.send('my_awesome_topic', b'a message from the revived producer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64219d71",
   "metadata": {},
   "source": [
    "Messages have a `<key, value>` pair data structure.\n",
    "\n",
    "So far we have produced only messages with a given `value` but a `key` can be added as well.\n",
    "(message keys can be used also to point messages to specific partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.send(topic='my_awesome_topic', key=b'some_key', value=b'a message with key')\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e702b6",
   "metadata": {},
   "source": [
    "## Create a topic from kafka-python\n",
    "\n",
    "Kafka-python allows to admin the kafka cluster by defining new topics, and assinging then specific configuration parameters, such as the replication factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87899de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "# connect to the cluster to run admin functions\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb1a9a",
   "metadata": {},
   "source": [
    "Let's check the list of topics present on the cluster.\n",
    "\n",
    "This is the equivalent of issuing `./kafka-topics.sh --list --bootstrap-server kafka-broker:9092`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc007dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74759c50",
   "metadata": {},
   "source": [
    "Topics are partitioned entities.\n",
    "Within each partition events are added to the end of the log, resulting in an ordered list of records.\n",
    "\n",
    "Publishing a new message to a partitioned topic will result in the addition of the message to the end of the log retained on the owner of a specific partition. If replication is enabled, the message will be then ridistributed to the other follower partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc69a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new topic explicitely\n",
    "#    w/   2 partitions\n",
    "#    w/o  replication \n",
    "a_new_topic = NewTopic(name='a_partitioned_topic', \n",
    "                       num_partitions=2, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[a_new_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc08a9e",
   "metadata": {},
   "source": [
    "## Publish messages for the Spark Structured Streaming example\n",
    "\n",
    "Kafka can be used as a source for incoming messages in Spark Streaming and Structured Streaming.\n",
    "\n",
    "In Spark 3.2.1 the kafka integration is unfortunately not available for pySpark Streaming (while is still available for scala and java).\n",
    "\n",
    "We'll use the pySpark Structured Streaming API for implementing the example previously seen in the Spark hands-on sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8306a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "first_names=('John','Andy','Joe','Alice')\n",
    "last_names=('Johnson','Smith','Jones', 'Millers')\n",
    "\n",
    "# while 1:\n",
    "for i in range(20):\n",
    "    msg = {\n",
    "        'name': random.choice(first_names),\n",
    "        'surname': random.choice(last_names),\n",
    "        'amount': '{:.2f}'.format(random.random()*1000),\n",
    "        'delta_t': '{:.2f}'.format(random.random()*10),\n",
    "        'flag': random.choices([0,1], weights=[0.8, 0.2])[0]\n",
    "    }\n",
    "    producer.send('a_partitioned_topic', json.dumps(msg).encode('utf-8'))\n",
    "    producer.flush()\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076f3da",
   "metadata": {},
   "source": [
    "Let's create a new topic where to store the results of the Kafka+Spark processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_new_topic = NewTopic(name='results', \n",
    "                       num_partitions=2, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[a_new_topic])\n",
    "\n",
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22da3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
